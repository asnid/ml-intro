{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2fJWG6zpB12"
   },
   "source": [
    "# Satellite Object Detection - Inference\n",
    "\n",
    "* Jupyter notebook to run inference on a [ModelZoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) trained model\n",
    "* Runs in Deep Learning VM on Google Compute Engine using [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) - compatible with 13 July 2018 updates ([release](https://github.com/tensorflow/models/tree/master/research/object_detection#july-13-2018))\n",
    "* Runs in a Python 2 notebook\n",
    "\n",
    "### Requirements:\n",
    "* Dataset in zip archive on Google Cloud Storage:\n",
    "```\n",
    "[dataset archive name].zip\n",
    "   |- rec [optional - will generate any files not present]\n",
    "      |- label_map.pbtxt\n",
    "      |- classes.txt\n",
    "   |- img_test\n",
    "      |- [image_class]_0.jpg\n",
    "      |- [image_class]_1.jpg\n",
    "      |- ...\n",
    "   |- xml_test\n",
    "      |- [image_class]_0.xml\n",
    "      |- [image_class]_1.xml\n",
    "      |- ...\n",
    "```\n",
    "\n",
    "* Tensorflow Object Detection API in zip archive on Google Drive:\n",
    "```\n",
    "[tf archive name].zip\n",
    "   |- object_detection\n",
    "      |- [Tensorflow model API files/folders]\n",
    "   |- slim\n",
    "      |- [Tensorflow model API files/folders]\n",
    "```\n",
    "\n",
    "### To make XML annotations from images:\n",
    "* Use [LabelImg](https://github.com/tzutalin/labelImg) for images with unknown boxes\n",
    "* Use `xml_make.py` for images synthesized in Unity dataset generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhJhjocdJtMo"
   },
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "gHWQhA9SGVwi",
    "outputId": "c805e21e-759e-49de-cd38-6a2acf852554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new directories...\n",
      "...Done creating directories\n",
      "\n",
      "Updating system paths...\n",
      "...Done updating paths\n"
     ]
    }
   ],
   "source": [
    "## This block deletes all files and folders, and starts from scratch\n",
    "## (akin to restarting the runtime)\n",
    "\n",
    "# -------- Parameters --------\n",
    "\n",
    "data_dir_name = 'data'  # Name of folder where images/annotations/models should be stored\n",
    "code_dir_name = 'src/od_api'  # Name of folder where Tensorflow Object Detection API should be stored\n",
    "model_dir_name = 'mdl'  # Name of folder where Tensorflow models should be stored\n",
    "util_dir_name = 'util'  # Name of folder where utility scripts should be stored\n",
    "\n",
    "delete_all_uploaded_files = False  # Start anew\n",
    "\n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "import sys\n",
    "import os, shutil\n",
    "import re\n",
    "\n",
    "def clean_slate(directory, keep_these, keep_these_exts=['.ipynb']):\n",
    "    \"\"\" Deletes subdirectories of a given directory, ignoring config files and such.\n",
    "    Args:\n",
    "        directory: Directory in which to delete subdirectories.\n",
    "        keep_these: Subdirectories/files to not delete.\n",
    "        keep_these_exts: File extensions to not delete.\n",
    "    \"\"\"\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    try:\n",
    "        os.chdir(directory)\n",
    "    except OSError as e:\n",
    "        print('Cannot access desired directory {}. Caught error: {}'.format(directory, str(e)))\n",
    "        return\n",
    "    \n",
    "    # Make sure given file extensions are lowercase and start with '.'\n",
    "    for ii in range(len(keep_these_exts)):\n",
    "        keep_these_exts[ii] = keep_these_exts[ii].lower()\n",
    "        if not keep_these_exts[ii].startswith('.'):\n",
    "            keep_these_exts[ii] = '.' + keep_these_exts[ii]\n",
    "    for ii in range(len(keep_these)):\n",
    "        keep_these[ii] = keep_these[ii].lower()\n",
    "    \n",
    "    print('Cleaning up ' + directory + '...')\n",
    "    \n",
    "    # Delete all unnecessary subdirectories\n",
    "    # (i.e., don't delete anything that starts with '.', such as '.config')\n",
    "    subdirs = next(os.walk('.'))[1]  # Get list of directories in working directory\n",
    "    for subdir in os.listdir(directory):\n",
    "        if not subdir.startswith('.') and subdir.lower() not in keep_these \\\n",
    "                                      and os.path.splitext(subdir)[1].lower() not in keep_these_exts:\n",
    "            try:\n",
    "                shutil.rmtree(subdir)\n",
    "            except OSError as e:\n",
    "                print('    Failed to remove {}. Caught error: {}'.format(os.path.join(directory,subdir), str(e)))\n",
    "            print('    Removed ' + os.path.join(directory,subdir))\n",
    "    print('...Done')\n",
    "    os.chdir(cwd)\n",
    "\n",
    "# -----\n",
    "\n",
    "# Directory names\n",
    "# \"replace\" to normalize OS path separator - in future blocks (now that os module is loaded), should only use os.path.join, not manual separator (e.g., 'this/is/a/path')\n",
    "normpath = lambda path : path.replace('/', os.path.sep).replace('\\\\', os.path.sep)\n",
    "\n",
    "WORKING_DIR = normpath('/home/jupyter/')  # Base folder in Jupyter instance (not root) - should already exist\n",
    "DATA_DIR = os.path.join(WORKING_DIR, normpath(data_dir_name))  # Where images, annotations, and models will be stored\n",
    "CODE_DIR = os.path.join(WORKING_DIR, normpath(code_dir_name))  # Where Tensorflow Object Detection API code will be stored\n",
    "MODEL_DIR = os.path.join(WORKING_DIR, normpath(model_dir_name))  # Where Tensorflow models will be stored\n",
    "UTIL_DIR = os.path.join(WORKING_DIR, normpath(util_dir_name))  # Where utility scripts will be stored\n",
    "\n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "os.chdir(WORKING_DIR)  # If this doesn't exist, then figure out what the actual working directory is and change the variable above\n",
    "\n",
    "# Delete old directories\n",
    "if delete_all_uploaded_files:\n",
    "    try: os.rename(os.path.join(WORKING_DIR, 'src', 'models'), os.path.join(WORKING_DIR, 'src', 'tf_api'))\n",
    "    except: pass\n",
    "    try: os.rename(os.path.join(WORKING_DIR, 'tutorials'), os.path.join(WORKING_DIR, 'test', 'gce_ai_demo'))\n",
    "    except: pass\n",
    "    \n",
    "    clean_slate(WORKING_DIR, keep_these=['test', 'src'], keep_these_exts=['.ipynb', '.py'])\n",
    "    clean_slate(os.path.join(WORKING_DIR, 'src'), keep_these=['tf_api', 'tensorflow', 'tpu'], keep_these_exts=['.ipynb', '.py'])\n",
    "\n",
    "# Make new directories\n",
    "print('Creating new directories...')\n",
    "for directory in [DATA_DIR, CODE_DIR, MODEL_DIR, UTIL_DIR]:\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "        print('    Made directory ' + directory)\n",
    "    except OSError:\n",
    "        pass  # Directory probably already exists\n",
    "print('...Done creating directories')\n",
    "\n",
    "# Add folders to system path\n",
    "print('\\nUpdating system paths...')\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")\n",
    "for directory in [WORKING_DIR, CODE_DIR]:\n",
    "    if directory not in sys.path:\n",
    "        sys.path.append(directory)\n",
    "        print('    Added ' + directory + ' to system path')\n",
    "print('...Done updating paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading utility methods\n"
     ]
    }
   ],
   "source": [
    "GCLOUD_BUCKET = 'csys-ssa1b'\n",
    "\n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "import sys\n",
    "import os, shutil\n",
    "import re\n",
    "\n",
    "\n",
    "# Methods for interfacing with Google Cloud Storage\n",
    "\n",
    "def get_file_from_gcloud(bucket_name, path_to_file, dir_to_save_in = None, archive_contents='archive'):\n",
    "    \"\"\" Transfers a file from a Google Cloud Storage bucket to the Jupyter instance.\n",
    "    Args:\n",
    "        bucket_name: Cloud Storage bucket to get file from.\n",
    "        path_to_file: Path to file in Cloud Storage bucket.        \n",
    "        dir_to_save_in: Directory within Jupyter instance in which to save file.\n",
    "        archive_contents: Name of archive (only used for console output).\n",
    "    \"\"\"\n",
    "    \n",
    "    if dir_to_save_in is None or dir_to_save_in == '':\n",
    "        dir_to_save_in = os.getcwd()\n",
    "    else:\n",
    "        try: os.makedirs(dir_to_save_in)\n",
    "        except OSError: pass\n",
    "    \n",
    "    print('Downloading {} (in file {} from Google Cloud Storage bucket {})...'.format(archive_contents, path_to_file, bucket_name))\n",
    "    print('=======================')\n",
    "    !gsutil cp -n gs://{bucket_name}/{path_to_file} {dir_to_save_in}\n",
    "    print('=======================')\n",
    "    print('...Done trying - check above lines for success\\n')\n",
    "\n",
    "def send_file_to_gcloud(path_to_file, bucket_name, dir_to_save_in = None):\n",
    "    \"\"\" Transfers a file from the Jupyter instance to a Google Cloud Storage bucket.\n",
    "    Note:\n",
    "        Doesn't currently work (permission error) - run the shell command from an SSH window instead,\n",
    "        after authenticating account with command  gcloud auth login [account_email].\n",
    "    Args:\n",
    "        path_to_file: Path to file in Jupyter instance.\n",
    "        bucket_name: Cloud Storage bucket to save in.\n",
    "        dir_to_save_in: Directory within Cloud Storage bucket in which to save file.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Uploading file {} to Google Cloud Storage bucket {}...'.format(path_to_file, bucketname))\n",
    "    print('=======================')\n",
    "    \n",
    "    if dir_to_save_in is None:\n",
    "        !gsutil cp {path_to_file} gs://{bucket_name}\n",
    "    else:\n",
    "        !gsutil cp {path_to_file} gs://{bucket_name}/{dir_to_save_in}\n",
    "    \n",
    "    print('=======================')\n",
    "    print('...Done trying - check above lines for success\\n')\n",
    "    \n",
    "    \n",
    "# Utility methods\n",
    "\n",
    "def split_top_path_level(path):\n",
    "    \"\"\" Gets the top object in an OS path (either top folder or filename, whichever is the rightmost entry in the path.)\n",
    "    Args:\n",
    "        path: Path in which to split off the top-level object.\n",
    "    Returns:\n",
    "        Array: [0] = base path excluding top-level object, [1] = top-level object\n",
    "    \"\"\"\n",
    "    top_obj = path.rstrip('/\\\\').rsplit('/',1)[-1].rsplit('\\\\',1)[-1]\n",
    "    base_path = path.rstrip('/\\\\').rstrip(top_obj).rstrip('/\\\\')\n",
    "    return [base_path, top_obj]\n",
    "\n",
    "def extract_archive(path_to_archive, dir_to_extract_in = None, delete_after_extracting = True):\n",
    "    \"\"\" Extracts a zip or tar archive into a folder in the current instance.\n",
    "    Args:\n",
    "        path_to_archive: Path to archive stored in the current instance.\n",
    "        dir_to_extract_in: Where to extract the archive.\n",
    "        delete_archive_after_extracting: Whether to keep or remove the original archive file.\n",
    "    Raises:\n",
    "        ValueError if the archive is not a .zip or .tar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make directory to extract to\n",
    "    if dir_to_extract_in is None or dir_to_extract_in == '':\n",
    "        dir_to_extract_in = os.getcwd()\n",
    "    else:\n",
    "        try: os.makedirs(dir_to_extract_in)\n",
    "        except OSError: pass\n",
    "    \n",
    "    # Get archive extension (or extensions, accounting for double extensions like .tar.gz)\n",
    "    archive_file_extensions = [os.path.splitext(path_to_archive)[1], os.path.splitext(os.path.splitext(path_to_archive)[0])[1]]\n",
    "\n",
    "    # Open archive\n",
    "    if any(ext == '.tar' for ext in archive_file_extensions):\n",
    "        import tarfile\n",
    "        arch = tarfile.open(path_to_archive)\n",
    "        print('Extracting tar archive {}... '.format(os.path.basename(path_to_archive))),\n",
    "    elif any(ext == '.zip' for ext in archive_file_extensions):\n",
    "        from zipfile import ZipFile\n",
    "        arch = ZipFile(path_to_archive)\n",
    "        print('Extracting zip archive {}... '.format(os.path.basename(path_to_archive))),\n",
    "    else:\n",
    "        raise ValueError('Archive extensions {} and {} not recognized in file {}'.format(archive_file_extensions[0], archive_file_extensions[1], path_to_archive))\n",
    "    \n",
    "    # Extract archive\n",
    "    arch.extractall(dir_to_extract_in)\n",
    "    arch.close()\n",
    "    print('Done! Extracted to ' + dir_to_extract_in)\n",
    "\n",
    "    # Delete original archive file\n",
    "    if delete_after_extracting:\n",
    "        os.remove(path_to_archive)\n",
    "        \n",
    "def download_archive_from_url(archive_file_url, dir_to_extract_in, archive_contents='archive'):\n",
    "    \"\"\" Downloads and extracts a tar or zip archive from somewhere in the cloud.\n",
    "    Args:\n",
    "        archive_file_url: URL location of archive\n",
    "        dir_to_extract_in: Where to extract archive (in local directory tree)\n",
    "        archive_contents: Name of archive (only used for console output)\n",
    "    \"\"\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir('/tmp')\n",
    "\n",
    "    # Download archive\n",
    "    import urllib\n",
    "    print('Getting {}...'.format(archive_contents))\n",
    "    print('    Downloading archive from {}... '.format(archive_file_url)),\n",
    "    archive_filename = archive_file_url.rsplit('/', 1)[-1]\n",
    "    opener = urllib.URLopener()\n",
    "    opener.retrieve(archive_file_url, archive_filename)\n",
    "    print('Done')\n",
    "\n",
    "    # Extract archive\n",
    "    print('   '),\n",
    "    extract_archive(archive_filename, dir_to_extract_in, delete_after_extracting = True)\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "    print('...Done getting {}'.format(archive_contents))\n",
    "    \n",
    "def archive_files_to_indexed_directory(dir_to_archive, append_str='old'):\n",
    "    \"\"\" Moves all files from a directory to an archive directory (at the same level).\n",
    "    Args:\n",
    "        dir_to_archive: Directory whose files to move to an archive\n",
    "        append_str: String to append to archive directory names (default: 'old')\n",
    "    Returns:\n",
    "        archive_dir: Name of (and path to) archive directory\n",
    "    \"\"\"\n",
    "\n",
    "    # Make top-level archive folder\n",
    "    [base_path, top_folder] = split_top_path_level(dir_to_archive)\n",
    "    archive_path = os.path.join(base_path, '{}_old'.format(top_folder))\n",
    "    try:\n",
    "        os.makedirs(archive_path)\n",
    "    except OSError:\n",
    "        pass  # Archive folder probably already exists  \n",
    "    \n",
    "    # Find the first index for which the folder '[append_str]_[idx]' doesn't exist yet within the archive directory\n",
    "    idx = 1\n",
    "    while True:\n",
    "        # Get the next potential directory name. If it already exists, increment the counter and try again\n",
    "        # If the directory doesn't already exist, we've found the right index and we're done\n",
    "        archive_dir = os.path.join(archive_path, '{}_{}'.format(append_str, idx))\n",
    "        if os.path.isdir(archive_dir):\n",
    "            idx += 1\n",
    "        else:\n",
    "            break  # Found an index for which the archive directory doesn't exist yet (so can use it now)\n",
    "\n",
    "    # Change the directory name to the archive name, then recreate the initial directory\n",
    "    os.rename(dir_to_archive, archive_dir)\n",
    "    os.makedirs(dir_to_archive)  # Will be empty\n",
    "\n",
    "    return archive_dir  # Return archived directory name\n",
    "\n",
    "def list_common_files(dirs, txt_output_path):\n",
    "    \"\"\" Makes a list of all filenames present in all given directories (excluding extension), and outputs list to a .txt file\n",
    "    Args:\n",
    "        dirs: Directories whose common files should have their names stored in list_path\n",
    "        txt_output_path: Path (including filename) where .txt file should be stored\n",
    "    Raises:\n",
    "        OSError if file txt_output_path already exists\n",
    "    Returns:\n",
    "        files_included: List of all filenames present in all given directories\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(txt_output_path):\n",
    "        raise OSError(\"File \" + txt_output_path + \" already exists - not creating anew\")\n",
    "\n",
    "    print('Getting list of files common to all given directories (saving to {})... '.format(txt_output_path))\n",
    "    \n",
    "    print('    Directories checked:')\n",
    "    files_in_each_dir = []\n",
    "    for directory in dirs:\n",
    "        try:\n",
    "            files_in_each_dir.append([fn.split('.')[0] for fn in os.listdir(directory)])  # Get filenames without extension\n",
    "            print('        Including directory ' + directory)\n",
    "        except OSError:\n",
    "            print('        Omitting directory ' + directory + ' - could not get file list. Was the full directory path given?')\n",
    "            \n",
    "\n",
    "    print('    Checking for common files... '),\n",
    "    files_included = []\n",
    "\n",
    "    with open(txt_output_path, 'w') as list_file:\n",
    "        all_files_found = True\n",
    "\n",
    "        for fn in files_in_each_dir[0]:\n",
    "            # If file is found in every given folder, add it to the list\n",
    "            if all(fn in files_in_each_dir[ii] for ii in range(len(dirs))):\n",
    "                list_file.write(fn + '\\n')\n",
    "                files_included.append(fn)\n",
    "\n",
    "            # Otherwise, omit it from the list (this block just gives console output)\n",
    "            else:\n",
    "                if all_files_found:\n",
    "                    all_files_found = False\n",
    "                    print(' ')\n",
    "                print('        Warning: could not find file {} in all of the above directories - excluding from list'.format(fn))\n",
    "\n",
    "        if all_files_found:\n",
    "            print('Done')\n",
    "        else:\n",
    "            print('    ...Saved list of common files, but excluded files not present in every directory (see above)')\n",
    "\n",
    "    print('...Got and saved list of files')\n",
    "    return files_included\n",
    "\n",
    "\n",
    "#  ========================================\n",
    "\n",
    "\n",
    "print('Done loading utility methods')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAVNrxnMLfRv"
   },
   "source": [
    "## Get Tensorflow Object Detection API \n",
    "from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "62jdOZtuG4Kw",
    "outputId": "53f6802a-dc27-4862-9984-30aba18988fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found object_detection subdirectory in /home/jupyter/src/object_detection_api - Tensorflow Object Detection API might already be downloaded! \n",
      "(If it needs to be replaced, remove current directory manually with shutil.rmtree.)\n",
      " \n",
      "Updating system paths...\n",
      "...Done updating paths\n"
     ]
    }
   ],
   "source": [
    "# OBJECT DETECTION API\n",
    "API_FN = 'code_tf-api-v1.14_training.zip'\n",
    "        \n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# Check if \"API-like\" files are already in the code directory\n",
    "for subdir in os.listdir(CODE_DIR):\n",
    "    if 'object_detection' in subdir or 'slim' in subdir:\n",
    "        print('Found {} subdirectory in {} - Tensorflow Object Detection API might already be downloaded! '.format(subdir, CODE_DIR)\n",
    "                  + '\\n(If it needs to be replaced, remove current directory manually with shutil.rmtree.)')\n",
    "        break\n",
    "# If not, download and extract the archive\n",
    "else:\n",
    "    check = raw_input('Download Object Detection API from Google Cloud Storage? (Will overwrite current API if present.) [y/n]')\n",
    "    if check.lower() == 'y':\n",
    "        get_file_from_gcloud(GCLOUD_BUCKET, API_FN, WORKING_DIR)\n",
    "        extract_archive(os.path.join(WORKING_DIR, API_FN), CODE_DIR)\n",
    "    else:\n",
    "        print('Not downloading API code anew - make sure it is present in {}, or else training will not run.'.format(CODE_DIR))\n",
    "\n",
    "# If cython is not installed, install it\n",
    "try:\n",
    "    if cython_installed:\n",
    "        pass\n",
    "except:\n",
    "    print('\\nInstalling Cython... '),\n",
    "    !pip install -q Cython\n",
    "    cython_installed = True\n",
    "    print('Done')\n",
    "    \n",
    "# If pycocotools is not installed, install it\n",
    "try:\n",
    "    if pycocotools_installed:\n",
    "        pass\n",
    "except:\n",
    "    # Find pycocotools setup file (somewhere in a subdirectory of 'src/object_detection_api/coco')\n",
    "    for subdir in os.listdir(CODE_DIR):\n",
    "        if 'coco' in subdir:\n",
    "            for root, _, fns in os.walk(os.path.join(CODE_DIR, subdir)):\n",
    "                if 'setup.py' in fns:\n",
    "                    try:\n",
    "                        print('Installing pycocotools...')\n",
    "                        print('===============================')\n",
    "                        cwd = os.getcwd()\n",
    "                        os.chdir(os.path.normpath(root))\n",
    "                        !python 'setup.py' install --user -q\n",
    "                        pycocotools_installed = True\n",
    "                    except:\n",
    "                        raise\n",
    "                    else:\n",
    "                        print('===============================')\n",
    "                        print('...Done installing pycocotools (succeeded if no errors between the \"=\" lines above)\\n')\n",
    "                        break\n",
    "                    finally:\n",
    "                        os.chdir(cwd)\n",
    "            else:\n",
    "                continue  # setup.py not found\n",
    "            break  # setup.py found and run\n",
    "    else:\n",
    "        raise RuntimeError('Could not find pycocotools!')\n",
    "    \n",
    "# Add Object Detection API folders to system path\n",
    "if 'PYTHONPATH' not in os.environ.keys():\n",
    "    os.environ['PYTHONPATH'] = ''  # CODE_DIR + ':' + os.path.join(CODE_DIR, 'object_detection') + ':' + os.path.join(CODE_DIR, 'slim')\n",
    "\n",
    "print(' ')\n",
    "print('Updating system paths...')\n",
    "os.chdir(CODE_DIR)\n",
    "for directory in os.listdir(CODE_DIR):  # Names of 'object_detection' and 'slim' directories\n",
    "    if os.path.isdir(directory) and not directory.startswith('.'):  # Make sure it's actually a directory (and not a file)\n",
    "        path_to_dir = os.path.join(CODE_DIR, directory)  # Full path to directory\n",
    "        if path_to_dir not in sys.path:\n",
    "            sys.path.append(path_to_dir)\n",
    "            print('    Added ' + path_to_dir + ' to system path')\n",
    "        if path_to_dir not in os.environ['PYTHONPATH']:\n",
    "            os.environ['PYTHONPATH'] += ':' + path_to_dir\n",
    "            print('    Added ' + path_to_dir + ' to Python path')\n",
    "os.chdir(WORKING_DIR)\n",
    "print('...Done updating paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY METHODS\n",
    "UTIL_FN = None\n",
    "        \n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# Figure out whether to download util script archive\n",
    "download_utils = False\n",
    "\n",
    "if UTIL_FN is not None and UTIL_FN != '':\n",
    "    # If util files are not already loaded, download them\n",
    "    if not os.path.exists(UTIL_DIR) or len(os.listdir(UTIL_DIR)) == 0:\n",
    "        download_utils = True\n",
    "    # If util files should be overwritten, download them\n",
    "    else:\n",
    "        check = raw_input('Utility methods found in {} - overwrite with archive from Google Cloud Storage? [y/n]')\n",
    "        if check.lower() == 'y':\n",
    "            download_utils = True\n",
    "            \n",
    "            try: shutil.rmtree(UTIL_DIR)\n",
    "            except OSError: pass\n",
    "            \n",
    "if download_utils:\n",
    "    get_file_from_gcloud(GCLOUD_BUCKET, UTIL_FN, WORKING_DIR)\n",
    "    extract_archive(os.path.join(WORKING_DIR, UTIL_FN), UTIL_DIR)\n",
    "else:\n",
    "    print('Not downloading util scripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfvbNE1yz_JB"
   },
   "source": [
    "## Get Trained Neural Net Model\n",
    "from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "DnnvCku10uoe",
    "outputId": "c16452dd-223c-4fc5-967f-fe27b5588c80"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e296888bf851>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e296888bf851>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    else\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# -------- Parameters --------\n",
    "\n",
    "# ML model location\n",
    "# Index: 0 = Frozen model in current Jupyter file structure (use iteration MODEL_STARTITER)\n",
    "#        1 = Google Cloud Storage (download model in bucket GCLOUD_BUCKET [earlier cell] at location MODEL_FN)\n",
    "model_location = 0\n",
    "\n",
    "# For model_location == 0:\n",
    "# Where to find inference graph <frozen_graph>.pb\n",
    "#     <MODEL_FRZN_DIR>/[<MODEL_ITER_PREFIX>_]<MODEL_ITER> (e.g., mdl/frzn/100000 or mdl/frzn/ssdv1_100000)\n",
    "# 0 = max available iteration\n",
    "MODEL_ITER = 0\n",
    "MODEL_ITER_PREFIX = None\n",
    "\n",
    "# For model_location == 1:\n",
    "# - Cloud Storage zip archive containing model with frozen inference graph (i.e., [saved_model].pb)\n",
    "MODEL_FN = 'model_ssdv1_20190920_frzn__iter57949.zip'\n",
    "\n",
    "\n",
    "# -----\n",
    "\n",
    "\n",
    "# Directory names\n",
    "MODEL_FRZN_DIR = os.path.join(MODEL_DIR, 'frzn')  # Exported model with frozen inference graph (after training)\n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "import os, shutil, glob\n",
    "\n",
    "# Find directory containing inference graph\n",
    "\n",
    "# If using model in current instance, get it\n",
    "if model_location == 0:    \n",
    "    # If using frozen model at max iteration, find it\n",
    "    if MODEL_ITER == 0:\n",
    "        for subdir in os.listdir(MODEL_FRZN_DIR):\n",
    "            try:\n",
    "                if MODEL_ITER_PREFIX is None or MODEL_ITER_PREFIX == '':\n",
    "                    this_iteration = int(subdir)\n",
    "                else:\n",
    "                    this_iteration = int(re.search(MODEL_ITER_PREFIX + r'_(\\d+)', s).group(1))\n",
    "            except (ValueError, AttributeError):\n",
    "                continue  # This subdirectory is not of the required form\n",
    "            else:\n",
    "                MODEL_ITER = max(MODEL_ITER, this_iteration)\n",
    "    \n",
    "    print('Getting directory for inference at iteration {}... '.format(MODEL_ITER)),\n",
    "    \n",
    "    # Get directory\n",
    "    if MODEL_ITER_PREFIX is None or MODEL_ITER_PREFIX == '':\n",
    "        model_graph_dir = os.path.join(MODEL_FRZN_DIR, str(MODEL_ITER))\n",
    "    else:\n",
    "        model_graph_dir = os.path.join(MODEL_FRZN_DIR, model_iter_prefix + '_' + str(MODEL_ITER))\n",
    "    \n",
    "    if os.path.exists(model_graph_dir):\n",
    "        print('Done! Using directory ' + model_graph_dir)\n",
    "    else:\n",
    "        raise OSError('Directory {} does not exist - cannot get graph to run inference'.format(model_graph_dir))    \n",
    "\n",
    "# If using model from Google Cloud Storage, download it\n",
    "elif model_location == 1:\n",
    "    model_graph_dir = os.path.join(MODEL_FRZN_DIR, 'inf')\n",
    "    get_file_from_gcloud(GCLOUD_BUCKET, MODEL_FN, WORKING_DIR, 'frozen Tensorflow model')\n",
    "    extract_archive(os.path.join(WORKING_DIR, MODEL_FN), model_graph_dir)\n",
    "    \n",
    "# Find graph\n",
    "print('Getting inference graph... ')\n",
    "for fn in os.listdir(model_graph_dir):\n",
    "    if fn.endswith('.pb'):\n",
    "        model_graph_file = os.path.join(model_graph_dir, fn)\n",
    "        print('Done! Using graph ' + model_graph_file)\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError('Graph (.pb) file not found in {} - cannot run inference'.format(model_graph_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EV33he32Ykuc"
   },
   "source": [
    "## Get Test Images and Annotations\n",
    "from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "OTuNWq4q7Ql1",
    "outputId": "ea7cbabd-169d-4cf0-f803-09f4cab01941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset already loaded - delete manually with shutil.rmtree to replace\n"
     ]
    }
   ],
   "source": [
    "# -------- Parameters --------\n",
    "\n",
    "# ID of zip archive containing image/annotation data (in Google Cloud Storage)\n",
    "DATASET_FN = (\n",
    "    'data_20190830_RSO.zip'\n",
    "    # 'data_20190830_RSO__sample357train.zip'\n",
    ")\n",
    "\n",
    "# Whether to delete current data and start again\n",
    "replace_data = False\n",
    "\n",
    "\n",
    "                                 \n",
    "# -----\n",
    "\n",
    "# Directory names\n",
    "REC_DIR = os.path.join(DATA_DIR, 'rec')  # Where label map is stored\n",
    "IMG_TEST_DIR = os.path.join(DATA_DIR, 'img_test')  # Where test images will be stored\n",
    "XML_TEST_DIR = os.path.join(DATA_DIR, 'xml_test')  # Where XML annotations for test images will be stored\n",
    "\n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "import os, shutil, glob\n",
    "\n",
    "def find_and_move_folder(from_within_dir, to_dir, file_exts, keyword=\"not_a_keyword\", dir_contents='\\b'):\n",
    "    \"\"\" Finds a folder containing files with a given extension, and moves it to a new location\n",
    "    Args:\n",
    "        from_within_dir: Directory in which to search for desired folder\n",
    "        to_dir: Where found folder should be moved\n",
    "        file_exts: Type of file that should be present in found folder (or list of potential file types - e.g., several image extensions)\n",
    "        keyword: If several folders with the desired file type are found, keyword denotes which directory should be taken here (e.g., \"train\" or \"test\" for image files)\n",
    "        dir_contents: What folder is being moved (only for console output)\n",
    "    Raises:\n",
    "        RuntimeError if record files are found in multiple folders in from_dir\n",
    "    \"\"\"\n",
    "\n",
    "    print('Moving {} folder... '.format(dir_contents)),\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(from_within_dir)\n",
    "    \n",
    "    # Make sure destination directory doesn't exist yet (archive it if it exists and is nonempty)\n",
    "    if os.path.isdir(to_dir):\n",
    "        if len(os.listdir(to_dir)) == 0:\n",
    "            shutil.rmtree(to_dir)\n",
    "        else:\n",
    "            archived_dir = archive_files_to_indexed_directory(to_dir)\n",
    "            print(' ')\n",
    "            print('    WARNING: destination directory {} is not empty - archived to {}'.format(to_dir, archived_dir))\n",
    "            print('...'),\n",
    "\n",
    "    # Parse file extensions\n",
    "    # If single string, make it a list\n",
    "    if file_exts == str(file_exts):\n",
    "        file_exts = [file_exts]\n",
    "    # Make extensions lower-case, and start with '.'\n",
    "    for ii in range(len(file_exts)):\n",
    "        file_exts[ii] = file_exts[ii].lower()\n",
    "        if not file_exts[ii].startswith('.'):\n",
    "            file_exts[ii] = '.' + file_exts[ii]            \n",
    "            \n",
    "    # Get all directories in from_dir containing desired file type\n",
    "    valid_dirs = []\n",
    "    for subdir in os.listdir(from_within_dir):\n",
    "        # Look for desired file type(s) in this folder\n",
    "        if any(len(glob.glob(os.path.join(subdir, '*{}*'.format(ext)))) > 0 for ext in file_exts):\n",
    "            valid_dirs.append(subdir)\n",
    "\n",
    "    # Check found directories\n",
    "    dir_found = False\n",
    "    # If only one directory contains desired file type, it's the right one\n",
    "    if len(valid_dirs) == 1:\n",
    "        try:\n",
    "            shutil.move(valid_dirs[0], to_dir)\n",
    "        except OSError as e:\n",
    "            print('Failed! Could not move {} to {}'.format(os.path.join(from_within_dir, valid_dirs[0]), to_dir))\n",
    "            raise e\n",
    "        else:\n",
    "            dir_found = True\n",
    "            print('Done! Moved {} to {}'.format(os.path.join(from_within_dir, valid_dirs[0]), to_dir))\n",
    "    # If multiple directories contain desired file type, look for one labelled with the given keyword\n",
    "    elif len(valid_dirs) > 1:\n",
    "        valid_dirs_with_keyword = [d for d in valid_dirs if keyword in d]\n",
    "        # If only one directory containing the desired file type has the keyword in its name, it's the right one\n",
    "        if len(valid_dirs_with_keyword) == 1:\n",
    "            try:\n",
    "                shutil.move(valid_dirs_with_keyword[0], to_dir)\n",
    "            except OSError as e:\n",
    "                print('Failed! Could not move {} to {}'.format(os.path.join(from_within_dir, valid_dirs_with_keyword[0]), to_dir))\n",
    "                raise e\n",
    "            else:\n",
    "                dir_found = True\n",
    "                print('Done! Moved {} to {}'.format(os.path.join(from_within_dir, valid_dirs_with_keyword[0]), to_dir))\n",
    "        # If multiple directories containing the desired file type have the keyword in their names, can't choose the right one\n",
    "        elif len(valid_dirs_with_keyword) > 1:\n",
    "            print('Failed! Cannot choose among {} folders with keyword \"{}\" - use shutil.move manually'.format(len(valid_dirs_with_keyword), keyword))\n",
    "\n",
    "        # If no directory containing the desired file type has the keyword in its name, can't choose the right one among the several initially-found directories\n",
    "        else:\n",
    "            print('Failed! Could not find folder with keyword \"{}\" within {} - use shutil.move manually'.format(keyword, from_within_dir))\n",
    "    # If no directories contain the desired file type, can't move anything\n",
    "    else:\n",
    "        print('Failed! No folder in {} contains {} files'.format(from_within_dir, file_exts))\n",
    "\n",
    "    if False:\n",
    "        if dir_found:\n",
    "            print('...Done moving {} folder'.format(dir_contents))\n",
    "        else:\n",
    "            print('...Done with errors')\n",
    "            raise OSError('Could not parse directory properly - see failures in comments above.')\n",
    "        \n",
    "    os.chdir(cwd)\n",
    "    \n",
    "# ====================================================\n",
    "\n",
    "\n",
    "ready_to_get_new_dataset = False\n",
    "got_dirs = {\n",
    "    'IMG_TEST': False,\n",
    "    'XML_TEST': False\n",
    "}\n",
    "\n",
    "# If replacing any current data, delete and remake data directory\n",
    "if replace_data:\n",
    "    try: shutil.rmtree(DATA_DIR)\n",
    "    except OSError: pass\n",
    "\n",
    "    try: os.makedirs(DATA_DIR)\n",
    "    except OSError: pass\n",
    "    \n",
    "    ready_to_get_new_dataset = True\n",
    "\n",
    "# If not replacing the current data, check if it seems to all be there\n",
    "# For each directory:\n",
    "#     - If directory name is None or '', then directory is not important for current run, so don't need it\n",
    "#     - If directory exists and is nonempty, then there is already data there so leave it as is\n",
    "else:\n",
    "    if IMG_TEST_DIR is None or IMG_TEST_DIR == '' or (os.path.exists(IMG_TEST_DIR) and len(os.listdir(IMG_TEST_DIR)) > 0):\n",
    "        got_dirs['IMG_TEST'] = True\n",
    "    if XML_TEST_DIR is None or XML_TEST_DIR == '' or (os.path.exists(XML_TEST_DIR) and len(os.listdir(XML_TEST_DIR)) > 0):\n",
    "        got_dirs['XML_TEST'] = True\n",
    "    if REC_DIR is None or REC_DIR == '' or (os.path.exists(REC_DIR) and len(os.listdir(REC_DIR)) > 0):\n",
    "        got_dirs['REC'] = True\n",
    "    \n",
    "    if all(dir_nonempty for dir_nonempty in got_dirs.values()):\n",
    "        print('Test data already loaded - delete manually with shutil.rmtree to replace')\n",
    "    else:\n",
    "        ready_to_get_new_dataset = True  # At least some required data is not present, so get the dataset and look for it\n",
    "        \n",
    "# Get new dataset\n",
    "if ready_to_get_new_dataset:\n",
    "    # Prep temp data directory for holding dataset files - if it already exists, delete it\n",
    "    DATA_TEMP_DIR = os.path.join(WORKING_DIR, 'data_temp')\n",
    "    try: shutil.rmtree(DATA_TEMP_DIR)\n",
    "    except OSError: pass\n",
    "    \n",
    "    # Load dataset into temp directory\n",
    "    get_file_from_gcloud(GCLOUD_BUCKET, DATASET_FN, WORKING_DIR, 'dataset')\n",
    "    extract_archive(os.path.join(WORKING_DIR, DATASET_FN), DATA_TEMP_DIR)\n",
    "    \n",
    "    # Move dataset parts to the \"default\" locations\n",
    "    print('\\nLooking for dataset folders...')\n",
    "    \n",
    "    if 'IMG_TEST' in got_dirs.keys() and not got_dirs['IMG_TEST']:\n",
    "        print('    '),\n",
    "        try: find_and_move_folder(DATA_TEMP_DIR, IMG_TEST_DIR, ['.jpg', '.jpeg', '.png', '.gif', '.tiff', '.bmp'], keyword='test', dir_contents='test image')\n",
    "        except OSError as e: print(\"Caught error: \" + repr(e))\n",
    "\n",
    "    if 'XML_TEST' in got_dirs.keys() and not got_dirs['XML_TEST']:\n",
    "        print('    '),\n",
    "        try: find_and_move_folder(DATA_TEMP_DIR, XML_TEST_DIR, '.xml', keyword='test', dir_contents='test annotation')\n",
    "        except OSError as e: print(\"Caught error: \" + repr(e))\n",
    "\n",
    "    if 'REC' in got_dirs.keys() and not got_dirs['REC']:\n",
    "        print('    '),\n",
    "        try: find_and_move_folder(DATA_TEMP_DIR, REC_DIR, '.record', keyword='rec', dir_contents='record')\n",
    "        except OSError as e: print(\"Caught error: \" + repr(e))\n",
    "\n",
    "    print('...Done looking for dataset folders')\n",
    "\n",
    "    # Delete temp directory\n",
    "    os.chdir(WORKING_DIR)\n",
    "    try: shutil.rmtree(DATA_TEMP_DIR)\n",
    "    except OSError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "SzH4tPXN0GQt",
    "outputId": "822a7f89-90ea-4eda-c335-3bb1914279e9"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-db03b5c447ea>, line 118)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-db03b5c447ea>\"\u001b[0;36m, line \u001b[0;32m118\u001b[0m\n\u001b[0;31m    if dict_key = 'id':\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if 'contextlib' not in sys.modules.keys():\n",
    "    print('Installing contextlib... ')\n",
    "    !pip install -q contextlib2\n",
    "    print('Done\\n')\n",
    "\n",
    "import os, shutil, glob\n",
    "import re\n",
    "import random\n",
    "import contextlib\n",
    "import tensorflow as tf\n",
    "import hashlib\n",
    "import io\n",
    "import PIL.Image\n",
    "from lxml import etree\n",
    "import importlib\n",
    "\n",
    "# Get label map utility module from Tensorflow Object Detection API\n",
    "os.chdir(CODE_DIR)\n",
    "for directory in os.listdir(CODE_DIR):\n",
    "    if 'utils' in os.listdir(directory):\n",
    "        dataset_util = importlib.import_module('.dataset_util', package=(directory + '.utils'))\n",
    "        label_map_util = importlib.import_module('.label_map_util', package=(directory + '.utils'))\n",
    "        break\n",
    "else:\n",
    "    print('WARNING: cannot find \"label_map_util.py\" in code directory - might cause failure later')\n",
    "os.chdir(WORKING_DIR)\n",
    "\n",
    "\n",
    "def find_label_map(directory):\n",
    "    pbtxts = [fn for fn in os.listdir(directory) if fn.endswith('.pbtxt')]\n",
    "    if len(pbtxts) == 1:\n",
    "        return os.path.join(pbtxts[0])\n",
    "    elif len(pbtxts) > 1:\n",
    "        for fn in pbtxts:\n",
    "            if 'label_map' in fn or 'labelmap' in fn:\n",
    "                return os.path.join(directory, fn)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "print('Getting necessary files in data directory...')\n",
    "    \n",
    "# Check data directory for file list and label map\n",
    "test_file_list = None\n",
    "label_map_in_data_dir = None\n",
    "try:\n",
    "    os.makedirs(REC_DIR)\n",
    "except OSError:\n",
    "    if os.path.isdir(REC_DIR) and 'test_files.txt' in os.listdir(REC_DIR):\n",
    "        test_file_list = dataset_util.read_examples_list(os.path.join(REC_DIR, 'test_files.txt'))\n",
    "        print('    Test file list found at {}'.format(os.path.join(REC_DIR, 'test_files.txt')))\n",
    "    \n",
    "    if os.path.exists(REC_DIR) and os.path.isdir(REC_DIR):\n",
    "        label_map_in_data_dir = find_label_map(REC_DIR)\n",
    "        if label_map_in_data_dir is not None:\n",
    "            print('    Label map found at {}'.format(label_map_in_data_dir))\n",
    "\n",
    "# Create txt file of training data filenames\n",
    "if test_file_list is None:\n",
    "    print('    Creating test file list... '),\n",
    "    try:\n",
    "        test_file_list = list_common_files([IMG_TEST_DIR, XML_TEST_DIR], os.path.join(REC_DIR, 'test_files.txt'))  # Returns list of objects in files\n",
    "        print('Done')\n",
    "    except OSError as e:\n",
    "        print('Failed!')\n",
    "        print('      Error creating file {}.'.format(os.path.join(REC_DIR, 'test_files.txt')))\n",
    "        print('      Error caught: ' + str(e))\n",
    "\n",
    "        \n",
    "# Get list of annotated objects contained in test annotations\n",
    "print('    Building dict of annotated objects... '),\n",
    "annotated_objects = dict()\n",
    "for fn in test_file_list:\n",
    "    # Add classes of all objects in image to annotated_objects dict (objects should have xml tag <name>)\n",
    "    xml_tree = etree.parse(os.path.join(XML_TEST_DIR, fn + '.xml'))\n",
    "    for elem in xml_tree.iter():\n",
    "        if elem.tag == 'name' and elem.text not in annotated_objects.keys():\n",
    "            annotated_objects[elem.text] = -1       \n",
    "# If label map is available, get ids of each annotated object name\n",
    "if label_map_in_data_dir is not None:\n",
    "    data_classes = label_map_util.get_label_map_dict(label_map_in_data_dir)\n",
    "    for label in annotated_objects.keys():\n",
    "        if label in data_classes.keys():\n",
    "            annotated_objects[label] = data_classes[label]  # Set id corresponding to label name\n",
    "print('Done! Found objects:' + '\\n      '.join([label for label in annotated_objects.keys()]))\n",
    "\n",
    "print('...Done getting files in data directory')\n",
    "\n",
    "\n",
    "# Check classes in label map from frozen model    \n",
    "print(' ')\n",
    "print('Checking label map...')\n",
    "\n",
    "if label_map_in_data_dir is None:\n",
    "    print('    WARNING: label map not found in data directory - data labels might not line up with model (check manually after inference)')\n",
    "\n",
    "label_map_in_model_dir = find_label_map(model_graph_dir)\n",
    "if label_map_in_model_dir is None:\n",
    "    print('    WARNING: label map not found in model directory - class ids might not correspond to data (check manually after inference)')\n",
    "else:\n",
    "    model_classes = label_map_util.get_label_map_dict(label_map_in_model_dir)\n",
    "    for label in annotated_objects.keys():\n",
    "        if label not in model_classes.keys():\n",
    "            print('    WARNING: inference graph is not looking for object {}, which is present in some images'.format(label))\n",
    "        elif annotated_objects[label] != model_classes[label]:\n",
    "            print('    WARNING: label {} has mismatched ids: {} in data and {} in model'.format(label, annotated_objects[label], model_classes[label]))\n",
    "        else:\n",
    "            print('    Label found: {} (id {})'.format(label, annotated_objects[label]))\n",
    "\n",
    "print('...Done checking label map. Look for any discrepancies above and fix manually (in model_classes dict)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAUGUWDoe38U"
   },
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hl6mCzbz8QKV",
    "outputId": "ac88292e-dc35-4197-abf4-b461309cc0f6"
   },
   "outputs": [],
   "source": [
    "output_image_width  = 25  # inches\n",
    "output_image_height = 25  # inches\n",
    "\n",
    "min_confidence = 1  # Only draw box if <min_confidence> percent sure that it's the right one\n",
    "max_boxes = 10  # Draw the <max_boxes> most likely boxes (as long as they're above <min_confidence>)\n",
    "\n",
    "max_num_img_to_infer = 0  # Will infer on [x] randomly-chosen images from full set of test images (0 = all available images)\n",
    "\n",
    "default_class_list = ['RSO']  # Overridden if model zip archive has a 'classes.txt' file\n",
    "\n",
    "\n",
    "# -------\n",
    "\n",
    "# Directory names\n",
    "import os\n",
    "OUT_BOX_DIR = os.path.join(DATA_DIR, 'out_test_box')\n",
    "\n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "import sys, os, shutil, glob\n",
    "import random\n",
    "import importlib\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "# Import object detection utilities from some folder in Object Detection API (search for it since folder structure could change)\n",
    "# If this code is implemented somewhere with a persistent directory structure (where the API location is known), can replace the following block with these two lines:\n",
    "#    from [object_detection_folder].utils import ops as ops_util\n",
    "os.chdir(CODE_DIR)\n",
    "for directory in os.listdir(CODE_DIR):\n",
    "    if 'utils' in os.listdir(directory):\n",
    "        ops_util = importlib.import_module('.ops', package=(directory + '.utils'))\n",
    "        break\n",
    "else:\n",
    "    print('WARNING: ops module not found - inference will probably fail')\n",
    "\n",
    "# Method to prep and detect objects in one image\n",
    "def run_inference_for_single_image(image, graph):\n",
    "    start_time = timer()\n",
    "\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in ['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes', 'detection_masks']:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "            \n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict, feed_dict={image_tensor: image})  # np.expand_dims(image, 0)})\n",
    "\n",
    "            # All outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "    \n",
    "    end_time = timer()\n",
    "    return output_dict, (end_time - start_time)\n",
    "\n",
    "# Method to get data from an image into a NumPy array\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "## SET UP\n",
    "\n",
    "run_inference = True\n",
    "\n",
    "# Get inference graph from model\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(model_graph_file, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "# Compile paths to test images\n",
    "print('Getting paths to test images... '),\n",
    "test_img_paths = []\n",
    "all_imgs_found = True\n",
    "num_test_imgs = 0\n",
    "for fn in test_files:\n",
    "    try:\n",
    "        test_img_paths.append(glob.glob(os.path.join(IMG_TEST_DIR, fn + '.*'))[0])\n",
    "        num_test_imgs += 1\n",
    "    except:\n",
    "        if all_imgs_found:\n",
    "            all_imgs_found = False\n",
    "            print('')\n",
    "        print('    Skipping {} - could not find image file in {}'.format(fn, IMG_TEST_DIR))\n",
    "\n",
    "print('Done!' if all_imgs_found else '...Done getting image paths.'),\n",
    "print('Found {} valid test images'.format(num_test_imgs))\n",
    "\n",
    "if (max_num_img_to_infer > 0 and max_num_img_to_infer < len(test_img_paths)):\n",
    "    test_img_paths = random.sample(test_img_paths, max_num_img_to_infer)\n",
    "num_test_imgs = len(test_img_paths)\n",
    "\n",
    "# Sidestep error with large image files\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Size of output images (inches)\n",
    "img_size = (output_image_width, output_image_height)\n",
    "\n",
    "# Array to hold inference times\n",
    "inference_times = []\n",
    "\n",
    "# Directory to output bounding box data\n",
    "try:\n",
    "    os.makedirs(OUT_BOX_DIR)\n",
    "except OSError:\n",
    "    check = raw_input('Output box directory {} is not empty. Continue? [y/n]'.format(OUT_BOX_DIR))\n",
    "    if check.lower() != 'y':\n",
    "        run_inference = False\n",
    "        print('Stopping run - handle the old output boxes, then rerun this cell')\n",
    "\n",
    "        \n",
    "## RUN INFERENCE\n",
    "\n",
    "if run_inference:\n",
    "    print(' ')\n",
    "    print('Running inference on ' + str(num_test_imgs) + ' images')\n",
    "    print('      - Min. score threshold to draw box: ' + str(0.01 * min_confidence))\n",
    "    print('      - Max. number of boxes drawn: ' + str(max_boxes))\n",
    "\n",
    "    for img_idx, img_path in enumerate(test_img_paths):\n",
    "        # If file is not an image, skip it\n",
    "        fn_ext = os.path.splitext(img_path)[1]\n",
    "        if fn_ext not in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']:\n",
    "            print(\"  ({}) Skipping {} - file extension {} not recognized as image\".format(img_idx + 1, os.path.basename(img_path), fn_ext))\n",
    "            continue\n",
    "\n",
    "        # Get image\n",
    "        image = Image.open(img_path)\n",
    "        image_np = load_image_into_numpy_array(image)  # Get array representation of image\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)  # Expand dimensions (model expects images to have shape [1, None, None, 3])\n",
    "\n",
    "        # Detect objects\n",
    "        output_dict, inference_time = run_inference_for_single_image(image_np_expanded, detection_graph)\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "        # Save detection boxes to file\n",
    "        fn_base = os.path.basename(img_path).rsplit('.', 1)[0]  # Without file extension\n",
    "        out_path = os.path.join(OUT_BOX_DIR, fn_base + '.txt')\n",
    "\n",
    "        try:\n",
    "            os.remove(out_path)  # If file exists, delete it (to be overwritten)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        with open(out_path, 'w+') as outfile:\n",
    "            box_str   = '[' + ','.join('[' + ','.join(str(sub_e) for sub_e in e) + ']' for e in output_dict['detection_boxes']) + ']'\n",
    "            class_str = '[' + ','.join(str(e) for e in output_dict['detection_classes']) + ']'\n",
    "            score_str = '[' + ','.join(str(e) for e in output_dict['detection_scores'])  + ']'\n",
    "\n",
    "            outfile.write(os.path.basename(img_path) + '\\n')\n",
    "            outfile.write('detection_boxes:\\n')\n",
    "            outfile.write(box_str)\n",
    "            outfile.write('\\ndetection_class_ids:\\n')\n",
    "            outfile.write(class_str)\n",
    "            outfile.write('\\ndetection_scores:\\n')\n",
    "            outfile.write(score_str)\n",
    "            outfile.write('\\ndetection_labels_indexed_to_id:\\n')\n",
    "\n",
    "            if (len(class_list) > 1):\n",
    "                outfile.write('[' + ','.join((\"'\" + str(e) + \"'\") for e in class_list) + ']')  # Labels by name (dummy 0 as first entry)\n",
    "            else:\n",
    "                outfile.write('[' + ','.join(str(e) for e in range(max(output_dict['detection_classes']) + 1)))  # Labels by numerical index\n",
    "\n",
    "            outfile.write('\\ninference_time_seconds:\\n')\n",
    "            outfile.write(str(inference_time))\n",
    "    \n",
    "        print(\"  ({}) Done {}\".format(img_idx + 1, os.path.basename(img_path)).ljust(70, ' ') + \" Time: {} s\".format(inference_time))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Udcn-RnPU9r6"
   },
   "source": [
    "## Freeze Trained Model\n",
    "and get zip archives ready to upload to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cp73hpU8ZrQ9",
    "outputId": "179c9246-4712-4204-bbed-4411d732b42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ready to export model...\n",
      "    Exporting model at iteration 57949\n",
      "    Exporting model to directory /home/jupyter/models/frzn/57949\n",
      "    Using config file /home/jupyter/models/init/pipeline.config\n",
      "    Exporting model using code file /home/jupyter/src/object_detection_api/object_detection/export_inference_graph.py\n",
      "...Done getting ready\n",
      " \n",
      "Exporting model...\n",
      "===================================================================================\n",
      "\n",
      "2019-09-23 15:05:54.128063: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/src/object_detection_api/object_detection/export_inference_graph.py\", line 108, in <module>\n",
      "    from object_detection import exporter\n",
      "ImportError: No module named object_detection\n",
      "\n",
      "===================================================================================\n",
      "...Export complete (if no errors between the \"=\" lines above)!\n",
      " \n",
      "Archiving exported model at iteration 57949 to /home/jupyter/models/frzn/zips...  Done\n",
      "  -- Upload to Google Cloud Storage by running command in SSH window:\n",
      "     \"gsutil cp /home/jupyter/models/frzn/zips/model_20190923_frzn__iter57949.zip gs://csys-ssa1b\"\n",
      " \n",
      "Archiving checkpoints to /home/jupyter/models/frzn/zips...  Done\n",
      "  -- Upload to Google Cloud Storage by running command in SSH window:\n",
      "     \"gsutil cp /home/jupyter/models/frzn/zips/model_20190923_ckpt__iter0-57949.zip gs://csys-ssa1b\"\n"
     ]
    }
   ],
   "source": [
    "# -------- Parameters --------\n",
    "\n",
    "export_iteration = 0  # Iteration at which to export graph (0 = export latest iteration)\n",
    "\n",
    "get_pbtxt = False  # Whether to get graph as a .pbtxt file in addition to .pb (ASCII-readable, but much larger)\n",
    "\n",
    "archive_model = True  # Whether to save frozen model into a zip archive\n",
    "archive_ckpts = True  # Whether to also save all available intermediate checkpoints into a zip archive\n",
    "\n",
    "\n",
    "# -----\n",
    "\n",
    "export_code_file = 'export_inference_graph.py'\n",
    "\n",
    "\n",
    "# =========================================\n",
    "\n",
    "\n",
    "def pb_from_pbtxt(path_to_pbtxt):\n",
    "    import tensorflow as tf\n",
    "    from google.protobuf import text_format\n",
    "\n",
    "    saved_graph_dir, pbtxt_fn = split_top_path_level(path_to_pbtxt)\n",
    "    pb_fn = os.path.splitext(pb_fn)[0] + '.pb'\n",
    "\n",
    "    with open(path_to_pbtxt, 'r') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        text_format.Merge(f.read(), graph_def)\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        tf.train.write_graph(graph_def, saved_graph_dir, pb_fn, as_text=False)\n",
    "    return os.path.join(saved_graph_dir, pb_fn)\n",
    "\n",
    "def pbtxt_from_pb(path_to_pb):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.platform import gfile\n",
    "\n",
    "    saved_graph_dir, pb_fn = split_top_path_level(path_to_pb)\n",
    "    pbtxt_fn = os.path.splitext(pb_fn)[0] + '.pbtxt'\n",
    "\n",
    "    with gfile.FastGFile(path_to_pb, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        tf.train.write_graph(graph_def, saved_graph_dir, pbtxt_fn, as_text=True)\n",
    "    return os.path.join(saved_graph_dir, pbtxt_fn)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "import os, glob\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "\n",
    "# PREP\n",
    "\n",
    "print('Getting ready to export model...')\n",
    "\n",
    "# Get iteration to export\n",
    "min_iteration = 99999999\n",
    "max_iteration = 0\n",
    "for fn in glob.glob(os.path.join(MODEL_CKPT_DIR, 'model.ckpt*.meta')):\n",
    "    this_iteration = int(re.search(r'model\\.ckpt-(\\d+)', fn).group(1))\n",
    "    if this_iteration > max_iteration: max_iteration = this_iteration\n",
    "    if this_iteration < min_iteration: min_iteration = this_iteration\n",
    "# If export_iteration == 0, use latest iteration\n",
    "if export_iteration == 0:\n",
    "    export_iteration = max_iteration\n",
    "\n",
    "print(\"    Exporting model at iteration {}\".format(export_iteration))\n",
    "\n",
    "# Try to create directory to export model to\n",
    "export_dir = os.path.join(MODEL_FRZN_DIR, str(export_iteration))\n",
    "try:\n",
    "    os.makedirs(export_dir)\n",
    "    print(\"    Exporting model to directory {}\".format(export_dir))\n",
    "except OSError as e:\n",
    "    if os.path.isdir(export_dir):\n",
    "        if len(os.listdir(export_dir)) != 0:\n",
    "            print(\"    Desired export directory {} is nonempty - archiving... \".format(export_dir))\n",
    "            export_archive_dir = archive_files_to_indexed_directory(export_dir)\n",
    "            os.makedirs(export_dir)\n",
    "            print(\"...Done archiving old files to {}\".format(export_archive_dir))\n",
    "        else:\n",
    "            pass  # Directory is empty, so it's fine to export there\n",
    "    else:\n",
    "        raise e  # Something else happened\n",
    "\n",
    "# Find config file in initial model directory\n",
    "config_files = glob.glob(os.path.join(MODEL_INIT_DIR, '*.config'))\n",
    "if len(config_files) == 1:\n",
    "    config_file = config_files[0]\n",
    "    print('    Using config file {}'.format(config_file))\n",
    "elif len(config_files) == 0:\n",
    "    raise RuntimeError('Cannot find a .config file in {}!'.format(MODEL_INIT_DIR))\n",
    "else:\n",
    "    raise RuntimeError('There are multiple .config files in {} - cannot decide which to use! (Files to choose from: {})'.format(MODEL_INIT_DIR, config_files))\n",
    "\n",
    "# Find export file in code directory\n",
    "os.chdir(CODE_DIR)\n",
    "for directory in os.listdir(CODE_DIR):\n",
    "    if export_code_file in os.listdir(directory):\n",
    "        export_model_fxn = os.path.join(CODE_DIR, directory, export_code_file)\n",
    "        print('    Exporting model using code file {}'.format(export_model_fxn))\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError('Could not find file {} in any first-level subdirectory of {} - cannot export model'.format(train_code_file, CODE_DIR))\n",
    "os.chdir(WORKING_DIR)\n",
    "\n",
    "print('...Done getting ready')\n",
    "\n",
    "\n",
    "# EXPORT INFERENCE GRAPH\n",
    "\n",
    "print(' ')\n",
    "print('Exporting model...')\n",
    "print('===================================================================================\\n')\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "trained_ckpt_prefix = os.path.join(MODEL_CKPT_DIR, 'model.ckpt-{}'.format(export_iteration))\n",
    "\n",
    "!python {export_model_fxn} \\\n",
    "      --input_type=image_tensor \\\n",
    "      --pipeline_config_path={config_file} \\\n",
    "      --output_directory={export_dir} \\\n",
    "      --trained_checkpoint_prefix={trained_ckpt_prefix}\n",
    "\n",
    "print('\\n===================================================================================')\n",
    "print('...Export complete (if no errors between the \"=\" lines above)!')\n",
    "\n",
    "\n",
    "# CONVERT GRAPH TO PBTXT\n",
    "\n",
    "if get_pbtxt:\n",
    "    print(' ')\n",
    "    print(\"Converting graph at iteration {} to pbtxt...\".format(export_iteration)),\n",
    "    try:\n",
    "        pbtxt_from_pb(os.path.join(MODEL_FRZN_DIR, str(export_iteration), 'frozen_inference_graph.pb'))\n",
    "    except:\n",
    "        print(\"Failed\")\n",
    "        raise\n",
    "    else:\n",
    "        print(\"Done\")\n",
    "\n",
    "\n",
    "# MAKE ARCHIVE WITH EXPORTED MODEL\n",
    "\n",
    "if archive_model:\n",
    "    # Make zip archive\n",
    "    try: os.makedirs(os.path.join(MODEL_FRZN_DIR, 'zips'))\n",
    "    except OSError: pass  # Directory probably already made\n",
    "\n",
    "    print(' ')\n",
    "    print('Archiving exported model at iteration {} to {}... '.format(export_iteration, os.path.join(MODEL_FRZN_DIR, 'zips'))),\n",
    "    now = datetime.now()\n",
    "    export_archive_path = os.path.join(MODEL_FRZN_DIR, 'zips', 'model_{}_frzn__iter{}'.format(now.strftime('%Y%m%d'), export_iteration))\n",
    "    shutil.make_archive(export_archive_path, 'zip', export_dir)  # Make archive with all files in exported data folder\n",
    "    print('Done')\n",
    "    print('  -- Upload to Google Cloud Storage by running command in SSH window:')\n",
    "    print('     \"gsutil cp {}.zip gs://{}\"'.format(export_archive_path, GCLOUD_BUCKET))\n",
    "\n",
    "if archive_ckpts:\n",
    "    print(' ')\n",
    "    print('Archiving checkpoints to {}... '.format(os.path.join(MODEL_FRZN_DIR, 'zips'))),\n",
    "    ckpt_archive_path = os.path.join(MODEL_FRZN_DIR, 'zips', 'model_{}_ckpt__iter{}-{}'.format(now.strftime('%Y%m%d'), min_iteration, max_iteration))\n",
    "    shutil.make_archive(ckpt_archive_path, 'zip', MODEL_CKPT_DIR)  # Make archive with all files in exported data folder\n",
    "    print('Done')\n",
    "    print('  -- Upload to Google Cloud Storage by running command in SSH window:')\n",
    "    print('     \"gsutil cp {}.zip gs://{}\"'.format(ckpt_archive_path, GCLOUD_BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2yZ3SyDKwl6"
   },
   "source": [
    "## Continue here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GOpn4IebMl6p"
   ],
   "name": "code_detection_training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
